## Общая структура всех моделей:
```python
keras.Sequential([...])  # Последовательная модель (слои идут один за другим)
model.compile(...)       # Настройка обучения (оптимизатор, функция потерь)
Каждая модель состоит из:
```
Входной слой - получает данные
Скрытые слои - обрабатывают информацию
Выходной слой - выдаёт результат (цену автомобиля)

### Модель 1: Простая сеть (базовая)
#### Архитектура:
Вход → [64 нейронов] → [32 нейрона] → Выход (1 нейрон)

### Модель 2: Глубокая сеть
#### Архитектура:
Вход → [128] → [64] → [32] → [16] → Выход

#### Особенности:
- Больше слоёв = больше возможностей для извлечения сложных признаков
- Иерархическое представление: каждый слой извлекает всё более абстрактные признаки
- Более медленное обучение, но потенциально более точное

### Модель 3: Широкая сеть
#### Архитектура:
Вход → [256 нейронов] → [128 нейронов] → Выход

#### Особенности:
- Больше нейронов в слоях = больше параллельной обработки
- Learning rate меньше (0.0005 вместо 0.001) - потому что большие сети требуют более осторожного обучения
- Быстрее извлекает признаки, но требует больше памяти

### Модель 4: Сеть с Dropout
#### Архитектура:
Вход → [128] → Dropout(30%) → [64] → Dropout(20%) → [32] → Выход

#### Dropout:
```python
layers.Dropout(0.3)  # 30% нейронов "выключаются" случайным образом
```
1. На каждой итерации обучения случайные нейроны временно удаляются
2. Остальные нейроны учатся быть более устойчивыми
3. На предсказании все нейроны работают

- Dropout используется для борьбы с переобучением, не давая нейронам сильно "привыкать" друг к другу, посольку некоторые нейроны отключаются на определенном слое. Это не даёт создавать "коалиции". 

- Также это улучшает обобщающую способность (модель становится более устойчивой), поскольку сеть становится стохастической (используются разные пути на разных итерациях, результат на одних и тех же данных не будет одинаковым).

- Поскольку на каждой эпохе случайные 30% нейронов отключены, модель автоматически производит ансамблирование.

### Модель 5: Сеть с BatchNormalization
#### Архитектура:
Вход → [128] → BatchNorm → [64] → BatchNorm → [32] → BatchNorm → Выход

#### Принцип работы нормализации:
1. Вычитает среднее
2. Делит на стандартное отклонение
3. Масштабирует и сдвигает (обучаемые параметры)

